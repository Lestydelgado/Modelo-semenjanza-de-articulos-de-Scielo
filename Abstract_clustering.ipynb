{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detección de Idiomas en el Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lectura de Librerías\n",
    "import pandas as pd\n",
    "import time\n",
    "#pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leyendo los abstracts\n",
    "df = pd.read_csv(r'Data/abstract_eng.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remover Stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ECSIM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import *\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se remueve el dato nan\n",
    "df = df[df['Abstract_ing'].isna()==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Abstracts = df[['Title','Abstract_ing']].drop_duplicates()\n",
    "Abstracts.reset_index(inplace=True)\n",
    "Abstracts[\"Abstract_ing\"] = Abstracts[\"Abstract_ing\"].str.lower().str.split()\n",
    "#Abstracts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01038124958674113\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "Abstracts['Clean_Abstract'] = Abstracts[['Abstract_ing']] \\\n",
    ".apply(lambda x: [item for item in x if item not in stop])\n",
    "end = time.time()\n",
    "print((end-start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Abstracts['Clean_Abstract'] = Abstracts['Abstract_ing'] \\\n",
    ".apply(lambda x: ' '.join([word for word in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The common way of doing this is to transform the documents into tf-idf vectors, \n",
    "then compute the cosine similarity between them. Any textbook on information retrieval (IR) covers this. See esp. Introduction to Information Retrieval, which is free and available online.\n",
    "\n",
    "Tf-idf (and similar text transformations) are implemented in the Python packages\n",
    "Gensim and scikit-learn. In the latter package, computing cosine similarities is as easy as\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1364099343617757\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "abstracts = Abstracts['Clean_Abstract']\n",
    "#stopwords_ = [word.decode('utf-8') for word in stopwords.words('english')]\n",
    "tfidf = TfidfVectorizer().fit_transform(abstracts)\n",
    "# no need to normalize, since Vectorizer will return normalized tf-idf\n",
    "end = time.time()\n",
    "print((end-start)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otras posibles Opciones\n",
    "https://stackoverflow.com/questions/51591510/text-similarity-approaches-do-not-reflect-real-similarity-between-texts\n",
    "https://stackoverflow.com/questions/8897593/how-to-compute-the-similarity-between-two-text-documents\n",
    "https://stackoverflow.com/questions/101569/algorithm-to-detect-similar-documents-in-python-script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Apuntes\n",
    "#https://stackoverflow.com/questions/25443802/unicode-warning-when-using-nltk-stopwords-with-tfidfvectorizer-of-scikit-learn\n",
    "https://hdbscan.readthedocs.io/en/latest/performance_and_scalability.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "temp_dict={}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "343.4885528008143\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "columnas = []\n",
    "for i in range(3,20,1):\n",
    "    if (i==13):\n",
    "        continue\n",
    "    nombre = \"C_\" + str(i)\n",
    "    columnas.append(nombre)\n",
    "    kmeans = KMeans(n_clusters=i).fit(tfidf)\n",
    "    temp_dict[nombre] = kmeans.labels_\n",
    "end = time.time()\n",
    "print((end-start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_category = pd.DataFrame(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp=pd.concat([Abstracts,df_category], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_temp = df_temp[['Title','C_10','C_11','C_12','C_13','C_14','C_15','C_16','C_17','C_18','C_19','C_20']]\n",
    "#df_final = df.merge(df_temp, how='right', on = 'Title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_final.drop(columns='Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.to_csv(r'Data/scopus_cluster2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans13 = KMeans(n_clusters=13).fit(tfidf)\n",
    "temp_dict['C_13'] = kmeans13.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C_3': array([0, 2, 0, ..., 0, 0, 2]),\n",
       " 'C_4': array([3, 0, 3, ..., 3, 3, 0]),\n",
       " 'C_5': array([0, 3, 0, ..., 0, 0, 4]),\n",
       " 'C_6': array([0, 1, 0, ..., 0, 0, 5]),\n",
       " 'C_7': array([6, 4, 6, ..., 0, 6, 2]),\n",
       " 'C_8': array([4, 7, 4, ..., 0, 4, 5]),\n",
       " 'C_9': array([3, 1, 3, ..., 7, 3, 2]),\n",
       " 'C_10': array([8, 2, 8, ..., 4, 8, 6]),\n",
       " 'C_11': array([0, 8, 1, ..., 1, 1, 7]),\n",
       " 'C_12': array([11,  6, 11, ...,  5, 11,  7]),\n",
       " 'C_14': array([0, 2, 1, ..., 1, 1, 1]),\n",
       " 'C_15': array([ 0,  2,  0, ..., 10,  0,  4]),\n",
       " 'C_16': array([12,  8,  9, ...,  9, 12,  9]),\n",
       " 'C_17': array([10, 13,  8, ..., 15,  8,  8]),\n",
       " 'C_18': array([ 2, 13, 12, ...,  9, 12, 12]),\n",
       " 'C_19': array([14,  6, 15, ..., 15, 15, 15]),\n",
       " 'C_13': array([12,  9,  7, ...,  7,  7,  7])}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
