{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detección de Idiomas en el Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lectura de Librerías\n",
    "import pandas as pd\n",
    "import time\n",
    "#pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leyendo los abstracts\n",
    "df = pd.read_csv(r'Data/abstract_eng.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remover Stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ECSIM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import *\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se remueve el dato nan\n",
    "df = df[df['Abstract_ing'].isna()==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Abstracts = df[['Title','Abstract_ing']].drop_duplicates()[0:5]\n",
    "Abstracts.reset_index(inplace=True)\n",
    "Abstracts[\"Abstract_ing\"] = Abstracts[\"Abstract_ing\"].str.lower().str.split()\n",
    "#Abstracts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00012604792912801106\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "Abstracts['Clean_Abstract'] = Abstracts[['Abstract_ing']] \\\n",
    ".apply(lambda x: [item for item in x if item not in stop])\n",
    "end = time.time()\n",
    "print((end-start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Abstracts['Clean_Abstract'] = Abstracts['Abstract_ing'] \\\n",
    ".apply(lambda x: ' '.join([word for word in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract_ing</th>\n",
       "      <th>Clean_Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>On Gymnodactylus amarali Barbour, 1925, with t...</td>\n",
       "      <td>[previously, considered, subspecies, g, wide, ...</td>\n",
       "      <td>previously considered subspecies g wide distri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Constraints to the implementation of effective...</td>\n",
       "      <td>[paper, addresses, limitations, scarcity, reli...</td>\n",
       "      <td>paper addresses limitations scarcity reliable ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Geochemical modeling of gold precipitation con...</td>\n",
       "      <td>[modeling, gold, deposition, performed, using,...</td>\n",
       "      <td>modeling gold deposition performed using softw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The specificity of interactions between protei...</td>\n",
       "      <td>[sulfated, polysaccharides, capable, binding, ...</td>\n",
       "      <td>sulfated polysaccharides capable binding prote...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>On the retrieval of significant wave heights f...</td>\n",
       "      <td>[synthetic, aperture, radar, satellites, sourc...</td>\n",
       "      <td>synthetic aperture radar satellites source dir...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                              Title  \\\n",
       "0      0  On Gymnodactylus amarali Barbour, 1925, with t...   \n",
       "1      1  Constraints to the implementation of effective...   \n",
       "2      2  Geochemical modeling of gold precipitation con...   \n",
       "3      3  The specificity of interactions between protei...   \n",
       "4      4  On the retrieval of significant wave heights f...   \n",
       "\n",
       "                                        Abstract_ing  \\\n",
       "0  [previously, considered, subspecies, g, wide, ...   \n",
       "1  [paper, addresses, limitations, scarcity, reli...   \n",
       "2  [modeling, gold, deposition, performed, using,...   \n",
       "3  [sulfated, polysaccharides, capable, binding, ...   \n",
       "4  [synthetic, aperture, radar, satellites, sourc...   \n",
       "\n",
       "                                      Clean_Abstract  \n",
       "0  previously considered subspecies g wide distri...  \n",
       "1  paper addresses limitations scarcity reliable ...  \n",
       "2  modeling gold deposition performed using softw...  \n",
       "3  sulfated polysaccharides capable binding prote...  \n",
       "4  synthetic aperture radar satellites source dir...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The common way of doing this is to transform the documents into tf-idf vectors, \n",
    "then compute the cosine similarity between them. Any textbook on information retrieval (IR) covers this. See esp. Introduction to Information Retrieval, which is free and available online.\n",
    "\n",
    "Tf-idf (and similar text transformations) are implemented in the Python packages\n",
    "Gensim and scikit-learn. In the latter package, computing cosine similarities is as easy as\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13650630712509154\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "abstracts = df['Abstract_ing']\n",
    "#stopwords_ = [word.decode('utf-8') for word in stopwords.words('english')]\n",
    "tfidf = TfidfVectorizer().fit_transform(abstracts)\n",
    "# no need to normalize, since Vectorizer will return normalized tf-idf\n",
    "end = time.time()\n",
    "print((end-start)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otras posibles Opciones\n",
    "https://stackoverflow.com/questions/51591510/text-similarity-approaches-do-not-reflect-real-similarity-between-texts\n",
    "https://stackoverflow.com/questions/8897593/how-to-compute-the-similarity-between-two-text-documents\n",
    "https://stackoverflow.com/questions/101569/algorithm-to-detect-similar-documents-in-python-script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Apuntes\n",
    "#https://stackoverflow.com/questions/25443802/unicode-warning-when-using-nltk-stopwords-with-tfidfvectorizer-of-scikit-learn\n",
    "https://hdbscan.readthedocs.io/en/latest/performance_and_scalability.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "temp_dict={}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "for i in range(3,20,1):\n",
    "    if (i==13):\n",
    "        continue\n",
    "    nombre = \"C_\" + str(i)\n",
    "    kmeans = KMeans(n_cluster=i)\n",
    "    temp_dict[nombre] = kmeans.labels_\n",
    "end = time.time()\n",
    "print((end-start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_category = pd.DataFrame(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp=pd.concat([Abstracts,df_category], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_temp = df_temp[['Title','C_10','C_11','C_12','C_13','C_14','C_15','C_16','C_17','C_18','C_19','C_20']]\n",
    "#df_final = df.merge(df_temp, how='right', on = 'Title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_final.drop(columns='Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_final.to_csv(r'Data/scopus_cluster.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
